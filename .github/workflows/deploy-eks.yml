# .github/workflows/deploy-eks.yml
name: Deploy EKS Cluster with OpenTofu

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'terraform/**'
      - '.github/workflows/deploy-eks.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - '.github/workflows/deploy-eks.yml'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'plan'
        type: choice
        options:
        - plan
        - apply
        - destroy
        - force-destroy
      confirm_apply:
        description: 'Type "yes" to confirm apply/destroy action'
        required: false
        default: 'no'
        type: string
      cluster_name:
        description: 'Override cluster name (optional)'
        required: false
        type: string
      cleanup_backend:
        description: 'Cleanup state backend after destroy'
        required: false
        default: 'false'
        type: choice
        options:
        - 'true'
        - 'false'

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: raiveton-test
  TF_VERSION: 1.6.6
  TOFU_VERSION: 1.6.0
  # Optional: override bucket name (leave empty for auto-generation)
  STATE_BUCKET_NAME: ""

jobs:
  workflow-info:
    name: Workflow Info
    runs-on: ubuntu-latest
    outputs:
      event-type: ${{ github.event_name }}
      is-manual: ${{ github.event_name == 'workflow_dispatch' }}
      is-push: ${{ github.event_name == 'push' }}
      is-pr: ${{ github.event_name == 'pull_request' }}
    
    steps:
    - name: Display Workflow Info
      run: |
        echo "## ğŸ” Workflow Information" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Event Type | ${{ github.event_name }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Repository | ${{ github.repository }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Branch | ${{ github.ref }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Actor | ${{ github.actor }} |" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "| Manual Action | ${{ github.event.inputs.action }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Confirm Apply | ${{ github.event.inputs.confirm_apply }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Custom Cluster | ${{ github.event.inputs.cluster_name }} |" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Expected Flow:" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event_name }}" = "push" ]; then
          echo "1. Setup State Backend" >> $GITHUB_STEP_SUMMARY
          echo "2. Plan (show changes)" >> $GITHUB_STEP_SUMMARY
          echo "3. Apply (if changes detected + manual approval)" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "1. Setup State Backend" >> $GITHUB_STEP_SUMMARY
          echo "2. Plan only (no apply)" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "1. Setup State Backend" >> $GITHUB_STEP_SUMMARY
          echo "2. ${{ github.event.inputs.action }} (with confirmations if needed)" >> $GITHUB_STEP_SUMMARY
        fi

  setup-state-backend:
    name: Setup State Backend
    runs-on: ubuntu-latest
    needs: workflow-info
    outputs:
      bucket-name: ${{ steps.create-bucket.outputs.bucket-name || steps.use-existing-bucket.outputs.bucket-name }}
      dynamodb-table: ${{ steps.create-dynamodb.outputs.table-name || steps.use-existing-dynamodb.outputs.table-name }}
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Generate bucket name
      id: generate-name
      run: |
        # Use custom bucket name if provided, otherwise generate from repo info
        if [ -n "${{ env.STATE_BUCKET_NAME }}" ]; then
          BUCKET_NAME="${{ env.STATE_BUCKET_NAME }}"
          echo "Using custom bucket name: ${BUCKET_NAME}"
        else
          # Create predictable bucket name based on repo and owner
          REPO_NAME=$(echo "${GITHUB_REPOSITORY}" | cut -d'/' -f2 | tr '[:upper:]' '[:lower:]')
          OWNER_NAME=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          
          # Create stable bucket name
          BUCKET_NAME="tofu-state2-${OWNER_NAME}-${REPO_NAME}"
          echo "Generated bucket name: ${BUCKET_NAME}"
        fi
        
        # Ensure it's valid (lowercase, no underscores, etc.)
        BUCKET_NAME=$(echo $BUCKET_NAME | sed 's/[^a-z0-9-]/-/g' | sed 's/--*/-/g')
        
        echo "bucket-name=${BUCKET_NAME}" >> $GITHUB_OUTPUT
        echo "dynamodb-table=${BUCKET_NAME}-lock" >> $GITHUB_OUTPUT
        echo "Final bucket name: ${BUCKET_NAME}"

    - name: Check if S3 bucket exists
      id: check-bucket
      run: |
        BUCKET_NAME="${{ steps.generate-name.outputs.bucket-name }}"
        
        # Check if bucket exists and is accessible
        if aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null; then
          echo "exists=true" >> $GITHUB_OUTPUT
          echo "âœ… Bucket $BUCKET_NAME already exists and is accessible"
        else
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "ğŸ“¦ Bucket $BUCKET_NAME does not exist, will create"
        fi

    - name: Create S3 bucket for Terraform state
      id: create-bucket
      if: steps.check-bucket.outputs.exists == 'false'
      run: |
        BUCKET_NAME="${{ steps.generate-name.outputs.bucket-name }}"
        
        # Create bucket with region-specific logic
        if [ "${{ env.AWS_REGION }}" = "us-east-1" ]; then
          # us-east-1 doesn't require LocationConstraint
          aws s3api create-bucket \
            --bucket $BUCKET_NAME \
            --region ${{ env.AWS_REGION }}
        else
          # All other regions require LocationConstraint
          aws s3api create-bucket \
            --bucket $BUCKET_NAME \
            --region ${{ env.AWS_REGION }} \
            --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}
        fi
        
        # Wait a moment for bucket to be fully created
        sleep 5
        
        # Verify bucket was created
        if aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null; then
          echo "âœ… Successfully created S3 bucket: $BUCKET_NAME"
        else
          echo "âŒ Failed to verify bucket creation"
          exit 1
        fi
        
        # Enable versioning
        aws s3api put-bucket-versioning \
          --bucket $BUCKET_NAME \
          --versioning-configuration Status=Enabled
        
        # Block public access
        aws s3api put-public-access-block \
          --bucket $BUCKET_NAME \
          --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
        
        # Enable encryption
        aws s3api put-bucket-encryption \
          --bucket $BUCKET_NAME \
          --server-side-encryption-configuration '{
            "Rules": [
              {
                "ApplyServerSideEncryptionByDefault": {
                  "SSEAlgorithm": "AES256"
                }
              }
            ]
          }'
        
        echo "bucket-name=${BUCKET_NAME}" >> $GITHUB_OUTPUT

    - name: Use existing bucket
      id: use-existing-bucket
      if: steps.check-bucket.outputs.exists == 'true'
      run: |
        BUCKET_NAME="${{ steps.generate-name.outputs.bucket-name }}"
        echo "bucket-name=${BUCKET_NAME}" >> $GITHUB_OUTPUT
        echo "â™»ï¸ Reusing existing S3 bucket: $BUCKET_NAME"
        
        # Verify bucket configuration
        echo "Checking bucket configuration..."
        aws s3api get-bucket-versioning --bucket $BUCKET_NAME || echo "Versioning status unknown"
        aws s3api get-bucket-encryption --bucket $BUCKET_NAME || echo "Encryption status unknown"

    - name: Check if DynamoDB table exists
      id: check-dynamodb
      run: |
        TABLE_NAME="${{ steps.generate-name.outputs.dynamodb-table }}"
        if aws dynamodb describe-table --table-name $TABLE_NAME 2>/dev/null; then
          echo "exists=true" >> $GITHUB_OUTPUT
        else
          echo "exists=false" >> $GITHUB_OUTPUT
        fi

    - name: Create DynamoDB table for state locking
      id: create-dynamodb
      if: steps.check-dynamodb.outputs.exists == 'false'
      run: |
        TABLE_NAME="${{ steps.generate-name.outputs.dynamodb-table }}"
        
        aws dynamodb create-table \
          --table-name $TABLE_NAME \
          --attribute-definitions AttributeName=LockID,AttributeType=S \
          --key-schema AttributeName=LockID,KeyType=HASH \
          --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
        
        # Wait for table to be active
        aws dynamodb wait table-exists --table-name $TABLE_NAME
        
        echo "table-name=${TABLE_NAME}" >> $GITHUB_OUTPUT
        echo "ğŸ”’ Created DynamoDB table: $TABLE_NAME"

    - name: Use existing DynamoDB table
      id: use-existing-dynamodb
      if: steps.check-dynamodb.outputs.exists == 'true'
      run: |
        TABLE_NAME="${{ steps.generate-name.outputs.dynamodb-table }}"
        echo "table-name=${TABLE_NAME}" >> $GITHUB_OUTPUT
        echo "â™»ï¸ Reusing existing DynamoDB table: $TABLE_NAME"

  plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend]
    if: |
      github.event_name == 'pull_request' || 
      (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'plan') ||
      (github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'))
    outputs:
      plan-exitcode: ${{ steps.plan.outputs.exitcode }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup OpenTofu
      uses: opentofu/setup-opentofu@v1
      with:
        tofu_version: ${{ env.TOFU_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create backend configuration
      run: |
        cat > terraform/backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${{ needs.setup-state-backend.outputs.bucket-name }}"
            key            = "eks-cluster/terraform.tfstate"
            region         = "${{ env.AWS_REGION }}"
            dynamodb_table = "${{ needs.setup-state-backend.outputs.dynamodb-table }}"
            encrypt        = true
          }
        }
        EOF

    - name: Terraform Init
      working-directory: ./terraform
      run: tofu init

    - name: Terraform Validate
      working-directory: ./terraform
      run: tofu validate

    - name: Terraform Plan
      id: plan
      working-directory: ./terraform
      run: |
        tofu plan \
          -var="cluster_name=${{ env.CLUSTER_NAME }}" \
          -var="region=${{ env.AWS_REGION }}" \
          -out=tfplan \
          -detailed-exitcode
      continue-on-error: true

    - name: Upload plan artifact
      uses: actions/upload-artifact@v4
      with:
        name: terraform-plan-${{ github.run_number }}
        path: terraform/tfplan
        retention-days: 5
        compression-level: 6

    - name: Create Plan Summary
      if: always()
      run: |
        echo "## ğŸ“‹ Terraform Plan Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.plan.outputs.exitcode }}" = "0" ]; then
          echo "âœ… **No changes detected** - Infrastructure is up to date" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ¯ No apply needed." >> $GITHUB_STEP_SUMMARY
        elif [ "${{ steps.plan.outputs.exitcode }}" = "2" ]; then
          echo "ğŸ“ **Changes detected** - Infrastructure will be modified" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸš€ Ready to apply changes!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps:" >> $GITHUB_STEP_SUMMARY
          echo "- Review the plan above carefully" >> $GITHUB_STEP_SUMMARY
          echo "- If this is a push to main/develop, the **Apply** job will wait for your approval" >> $GITHUB_STEP_SUMMARY
          echo "- Go to the **Apply** job and click **Review deployments** â†’ **Approve and deploy**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Plan failed** - Please check the logs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ” Check the plan output above for errors." >> $GITHUB_STEP_SUMMARY
        fi

  apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend, plan]
    if: |
      needs.plan.outputs.plan-exitcode == '2' && (
        (github.ref == 'refs/heads/main' && github.event_name == 'push') ||
        (github.ref == 'refs/heads/develop' && github.event_name == 'push') ||
        (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply')
      )
    environment: 
      name: production
      url: ${{ steps.cluster-info.outputs.cluster-endpoint }}
    
    steps:
    - name: Manual Approval Required
      run: |
        echo "ğŸ¯ **Manual Approval Required**"
        echo ""
        echo "ğŸ“‹ **Plan Summary:**"
        echo "- Plan detected changes (exit code: ${{ needs.plan.outputs.plan-exitcode }})"
        echo "- Infrastructure will be modified"
        echo ""
        echo "â³ **Waiting for approval...**"
        echo "Someone with approval rights needs to:"
        echo "1. Review the plan output above"
        echo "2. Click 'Review deployments' button"
        echo "3. Select 'production' environment" 
        echo "4. Click 'Approve and deploy'"
        echo ""
        echo "ğŸ›¡ï¸ This ensures no infrastructure changes happen without human review."

    - name: Validate manual confirmation
      if: github.event_name == 'workflow_dispatch'
      run: |
        echo "Debug info:"
        echo "Action: '${{ github.event.inputs.action }}'"
        echo "Confirm apply: '${{ github.event.inputs.confirm_apply }}'"
        echo "Event name: '${{ github.event_name }}'"
        echo ""
        
        if [ "${{ github.event.inputs.action }}" = "apply" ] && [ "${{ github.event.inputs.confirm_apply }}" != "yes" ]; then
          echo "âŒ Apply action requires confirmation. Please set 'confirm_apply' to 'yes'"
          echo "This safety check prevents accidental infrastructure changes."
          exit 1
        fi
        echo "âœ… Manual confirmation validated"

    - name: Push Event Info
      if: github.event_name == 'push'
      run: |
        echo "ğŸ”„ **Push Event Detected**"
        echo "Branch: ${{ github.ref }}"
        echo "Event: ${{ github.event_name }}"
        echo "No manual confirmation needed for push events - using Environment protection instead"

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup OpenTofu
      uses: opentofu/setup-opentofu@v1
      with:
        tofu_version: ${{ env.TOFU_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Download plan artifact
      uses: actions/download-artifact@v4
      with:
        name: terraform-plan-${{ github.run_number }}
        path: terraform/
      continue-on-error: true

    - name: Set cluster name
      id: set-cluster-name
      run: |
        if [ -n "${{ github.event.inputs.cluster_name }}" ]; then
          CLUSTER_NAME="${{ github.event.inputs.cluster_name }}"
          echo "Using custom cluster name: $CLUSTER_NAME"
        else
          CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
          echo "Using default cluster name: $CLUSTER_NAME"
        fi
        echo "cluster-name=${CLUSTER_NAME}" >> $GITHUB_OUTPUT

    - name: Create backend configuration
      run: |
        cat > terraform/backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${{ needs.setup-state-backend.outputs.bucket-name }}"
            key            = "eks-cluster/terraform.tfstate"
            region         = "${{ env.AWS_REGION }}"
            dynamodb_table = "${{ needs.setup-state-backend.outputs.dynamodb-table }}"
            encrypt        = true
          }
        }
        EOF

    - name: Terraform Init
      working-directory: ./terraform
      run: tofu init

    - name: Show Terraform State Before Apply
      working-directory: ./terraform
      run: |
        echo "ğŸ“Š Current Terraform State:"
        tofu state list || echo "No state found (fresh deployment)"
        echo ""

    - name: Terraform Apply
      working-directory: ./terraform
      run: |
        # Set environment variables for better output
        export TF_IN_AUTOMATION=1
        export TF_CLI_ARGS_apply="-parallelism=10"
        
        # Try to use saved plan first, fallback to new plan if needed
        if [ -f "tfplan" ]; then
          echo "ğŸ“¦ Using saved plan from previous step"
          echo "ğŸš€ Starting apply process..."
          echo "â±ï¸  This may take 15-20 minutes for EKS cluster creation..."
          echo ""
          
          # Apply with progress indicators
          tofu apply -no-color tfplan | while IFS= read -r line; do
            echo "$line"
            # Add timestamps to resource creation
            if [[ $line == *"Creating..."* ]] || [[ $line == *"Creation complete"* ]] || [[ $line == *"Still creating"* ]]; then
              echo "  â””â”€ [$(date '+%H:%M:%S')] $line" >&2
            fi
          done
          
          echo ""
          echo "âœ… Apply completed successfully!"
        else
          echo "ğŸ“ Creating fresh plan (saved plan not found)"
          echo "ğŸš€ Starting fresh apply process..."
          echo "â±ï¸  This may take 15-20 minutes for EKS cluster creation..."
          echo ""
          
          # Apply with progress indicators
          tofu apply \
            -var="cluster_name=${{ steps.set-cluster-name.outputs.cluster-name }}" \
            -var="region=${{ env.AWS_REGION }}" \
            -auto-approve \
            -no-color | while IFS= read -r line; do
            echo "$line"
            # Add timestamps to resource creation
            if [[ $line == *"Creating..."* ]] || [[ $line == *"Creation complete"* ]] || [[ $line == *"Still creating"* ]]; then
              echo "  â””â”€ [$(date '+%H:%M:%S')] $line" >&2
            fi
          done
          
          echo ""
          echo "âœ… Fresh apply completed successfully!"
        fi

    - name: Show Created Resources
      working-directory: ./terraform
      if: always()
      run: |
        echo "ğŸ“‹ Resources in Terraform State:"
        tofu state list | sort
        echo ""
        echo "ğŸ” Key Resource Details:"
        
        # EKS Cluster
        if tofu state show aws_eks_cluster.eks_cluster >/dev/null 2>&1; then
          echo "âœ… EKS Cluster found in state"
          tofu state show aws_eks_cluster.eks_cluster | grep -E "(id|arn|endpoint|status)"
        else
          echo "âŒ EKS cluster not found in state"
        fi
        
        # ArgoCD Namespace
        if tofu state show kubernetes_namespace.argocd >/dev/null 2>&1; then
          echo "âœ… ArgoCD namespace found in state"
          tofu state show kubernetes_namespace.argocd | head -10
        else
          echo "âŒ ArgoCD namespace not found in state"
        fi
        
        # Test connection ConfigMap
        if tofu state show kubernetes_config_map.test_connection >/dev/null 2>&1; then
          echo "âœ… Test connection ConfigMap found in state"
        else
          echo "âŒ Test connection ConfigMap not found in state"
        fi
        
        # ArgoCD Helm Release
        if tofu state show helm_release.argocd >/dev/null 2>&1; then
          echo "âœ… ArgoCD Helm release found in state"
          tofu state show helm_release.argocd | grep -E "(name|namespace|status)"
        else
          echo "âŒ ArgoCD Helm release not found in state"
        fi

    - name: Get cluster info
      id: cluster-info
      working-directory: ./terraform
      run: |
        CLUSTER_ENDPOINT=$(tofu output -raw cluster_endpoint)
        CLUSTER_NAME=$(tofu output -raw cluster_id)
        
        echo "cluster-endpoint=${CLUSTER_ENDPOINT}" >> $GITHUB_OUTPUT
        echo "cluster-name=${CLUSTER_NAME}" >> $GITHUB_OUTPUT
        
        # Wait for ArgoCD LoadBalancer to be ready
        echo "â³ Waiting for ArgoCD LoadBalancer to be ready..."
        
        # Try to get ArgoCD info with error handling
        ARGOCD_URL="pending"
        ARGOCD_PASSWORD="admin123"
        
        # Check if ArgoCD outputs exist
        if tofu output argocd_server_url >/dev/null 2>&1; then
          ARGOCD_URL=$(tofu output -raw argocd_server_url 2>/dev/null || echo "pending")
        fi
        
        if tofu output argocd_admin_password >/dev/null 2>&1; then
          ARGOCD_PASSWORD=$(tofu output -raw argocd_admin_password 2>/dev/null || echo "admin123")
        fi
        
        echo "argocd-url=${ARGOCD_URL}" >> $GITHUB_OUTPUT
        echo "argocd-password=${ARGOCD_PASSWORD}" >> $GITHUB_OUTPUT
        
        echo "Initial ArgoCD URL: ${ARGOCD_URL}"

    - name: Wait for ArgoCD LoadBalancer
      run: |
        echo "â³ Waiting for ArgoCD LoadBalancer to get external IP..."
        echo "This may take up to 5 minutes..."
        
        # Configure kubectl
        aws eks --region ${{ env.AWS_REGION }} update-kubeconfig --name ${{ steps.cluster-info.outputs.cluster-name }}
        
        # Wait for ArgoCD service to exist
        echo "ğŸ” Checking if ArgoCD service exists..."
        kubectl wait --for=condition=Ready --timeout=300s pod -l app.kubernetes.io/name=argocd-server -n argocd || echo "Pods not ready yet"
        
        # Wait for LoadBalancer to get external hostname
        echo "ğŸ” Waiting for LoadBalancer external hostname..."
        timeout=300
        counter=0
        while [ $counter -lt $timeout ]; do
          LB_HOST=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$LB_HOST" ] && [ "$LB_HOST" != "null" ]; then
            echo "âœ… LoadBalancer ready: $LB_HOST"
            echo "argocd-lb-host=${LB_HOST}" >> $GITHUB_OUTPUT
            break
          fi
          echo "â³ Still waiting... (${counter}s/${timeout}s)"
          sleep 15
          counter=$((counter + 15))
        done
        
        if [ $counter -ge $timeout ]; then
          echo "âš ï¸ LoadBalancer not ready after 5 minutes"
          echo "argocd-lb-host=pending" >> $GITHUB_OUTPUT
        fi
      id: wait-argocd

    - name: Test cluster connectivity
      run: |
        echo "ğŸ” Testing cluster connectivity..."
        
        # Configure kubectl
        aws eks --region ${{ env.AWS_REGION }} update-kubeconfig --name ${{ steps.cluster-info.outputs.cluster-name }}
        
        # Test basic connectivity
        echo "ğŸ“‹ Cluster info:"
        kubectl cluster-info
        
        echo ""
        echo "ğŸ“‹ Nodes:"
        kubectl get nodes -o wide
        
        echo ""
        echo "ğŸ“‹ System namespaces:"
        kubectl get namespaces
        
        echo ""
        echo "ğŸ“‹ All pods in kube-system:"
        kubectl get pods -n kube-system
        
        echo ""
        echo "ğŸ” Checking for ArgoCD namespace..."
        if kubectl get namespace argocd 2>/dev/null; then
          echo "âœ… ArgoCD namespace exists"
          echo "ğŸ“‹ ArgoCD pods:"
          kubectl get pods -n argocd -o wide
          echo ""
          echo "ğŸ“‹ ArgoCD services:"
          kubectl get svc -n argocd
        else
          echo "âŒ ArgoCD namespace not found!"
          echo "ğŸ”§ Let's create it manually..."
          kubectl create namespace argocd
          echo "âœ… ArgoCD namespace created manually"
        fi
        
        echo ""
        echo "ğŸ“‹ Final namespace list:"
        kubectl get namespaces

    - name: Create deployment summary
      run: |
        echo "## ğŸš€ EKS Cluster Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âœ… **Successfully deployed!**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ¯ Cluster Information" >> $GITHUB_STEP_SUMMARY
        echo "| Resource | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Cluster Name | ${{ steps.cluster-info.outputs.cluster-name }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Cluster Endpoint | ${{ steps.cluster-info.outputs.cluster-endpoint }} |" >> $GITHUB_STEP_SUMMARY
        echo "| AWS Region | ${{ env.AWS_REGION }} |" >> $GITHUB_STEP_SUMMARY
        echo "| State Bucket | ${{ needs.setup-state-backend.outputs.bucket-name }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ¨ ArgoCD Information" >> $GITHUB_STEP_SUMMARY
        echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        
        # Check if LoadBalancer is ready
        if [ "${{ steps.wait-argocd.outputs.argocd-lb-host }}" != "pending" ] && [ -n "${{ steps.wait-argocd.outputs.argocd-lb-host }}" ]; then
          ARGOCD_URL="http://${{ steps.wait-argocd.outputs.argocd-lb-host }}"
          echo "| ArgoCD URL | $ARGOCD_URL |" >> $GITHUB_STEP_SUMMARY
          echo "| Username | admin |" >> $GITHUB_STEP_SUMMARY
          echo "| Password | ${{ steps.cluster-info.outputs.argocd-password }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Status | âœ… Ready |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| ArgoCD URL | â³ LoadBalancer pending |" >> $GITHUB_STEP_SUMMARY
          echo "| Username | admin |" >> $GITHUB_STEP_SUMMARY
          echo "| Password | ${{ steps.cluster-info.outputs.argocd-password }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Status | â³ Setting up LoadBalancer... |" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“‹ Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. **Configure kubectl locally:**" >> $GITHUB_STEP_SUMMARY
        echo '```bash' >> $GITHUB_STEP_SUMMARY
        echo "aws eks --region ${{ env.AWS_REGION }} update-kubeconfig --name ${{ steps.cluster-info.outputs.cluster-name }}" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.wait-argocd.outputs.argocd-lb-host }}" != "pending" ] && [ -n "${{ steps.wait-argocd.outputs.argocd-lb-host }}" ]; then
          ARGOCD_URL="http://${{ steps.wait-argocd.outputs.argocd-lb-host }}"
          echo "2. **Access ArgoCD (Ready!):**" >> $GITHUB_STEP_SUMMARY
          echo "   - URL: $ARGOCD_URL" >> $GITHUB_STEP_SUMMARY
          echo "   - Username: admin" >> $GITHUB_STEP_SUMMARY
          echo "   - Password: ${{ steps.cluster-info.outputs.argocd-password }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "3. **ArgoCD CLI:**" >> $GITHUB_STEP_SUMMARY
          echo '```bash' >> $GITHUB_STEP_SUMMARY
          echo "# Install ArgoCD CLI" >> $GITHUB_STEP_SUMMARY
          echo "brew install argocd" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "# Login" >> $GITHUB_STEP_SUMMARY
          echo "argocd login ${{ steps.wait-argocd.outputs.argocd-lb-host }} --username admin --password ${{ steps.cluster-info.outputs.argocd-password }} --insecure" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        else
          echo "2. **ArgoCD LoadBalancer Setup:**" >> $GITHUB_STEP_SUMMARY
          echo "   - LoadBalancer is still being provisioned" >> $GITHUB_STEP_SUMMARY
          echo "   - Check status with:" >> $GITHUB_STEP_SUMMARY
          echo '```bash' >> $GITHUB_STEP_SUMMARY
          echo "kubectl get svc argocd-server -n argocd" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "   - URL will be available in a few minutes" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âš ï¸ **Important**: Change the default admin password after first login!"

  force-destroy:
    name: Force Destroy (AWS Direct)
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'force-destroy'
    environment: 
      name: production-destroy
      url: "Force destroying EKS cluster via AWS CLI"
    
    steps:
    - name: Validate destroy confirmation
      run: |
        if [ "${{ github.event.inputs.confirm_apply }}" != "yes" ]; then
          echo "âŒ Force destroy requires confirmation. Please set 'confirm_apply' to 'yes'"
          echo "âš ï¸  This will force delete EKS cluster and resources via AWS CLI!"
          exit 1
        fi
        echo "âœ… Force destroy confirmation validated"

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Force delete EKS resources
      run: |
        CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
        
        echo "ğŸ” Checking for existing EKS cluster..."
        if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
          echo "ğŸ“‹ Found cluster $CLUSTER_NAME, proceeding with force deletion..."
          
          # Delete node groups first
          echo "ğŸ—‘ï¸ Deleting node groups..."
          NODE_GROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'nodegroups[]' --output text)
          for ng in $NODE_GROUPS; do
            echo "Deleting node group: $ng"
            aws eks delete-nodegroup --cluster-name $CLUSTER_NAME --nodegroup-name $ng --region ${{ env.AWS_REGION }}
          done
          
          # Wait for node groups to delete
          echo "â³ Waiting for node groups to delete..."
          for ng in $NODE_GROUPS; do
            aws eks wait nodegroup-deleted --cluster-name $CLUSTER_NAME --nodegroup-name $ng --region ${{ env.AWS_REGION }}
          done
          
          # Delete cluster
          echo "ğŸ—‘ï¸ Deleting EKS cluster..."
          aws eks delete-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          aws eks wait cluster-deleted --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          
          echo "âœ… EKS cluster deleted successfully"
        else
          echo "â„¹ï¸ No EKS cluster found with name $CLUSTER_NAME"
        fi

    - name: Clean state backend
      if: github.event.inputs.cleanup_backend == 'true'
      run: |
        echo "ğŸ—‘ï¸ Cleaning state backend..."
        aws s3 rm s3://${{ needs.setup-state-backend.outputs.bucket-name }}/eks-cluster/ --recursive
        echo "âœ… State cleaned"

  destroy:
    name: Terraform Destroy
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    environment: 
      name: production-destroy
      url: "Destroying EKS cluster and all resources"
    
    steps:
    - name: Validate destroy confirmation
      run: |
        echo "Debug info:"
        echo "Action: '${{ github.event.inputs.action }}'"
        echo "Confirm apply: '${{ github.event.inputs.confirm_apply }}'"
        echo "Event name: '${{ github.event_name }}'"
        echo ""
        
        if [ "${{ github.event.inputs.confirm_apply }}" != "yes" ]; then
          echo "âŒ Destroy action requires confirmation. Please set 'confirm_apply' to 'yes'"
          echo "âš ï¸  This will permanently delete your EKS cluster and all associated resources!"
          echo "ğŸ’¡ To confirm, run the workflow again with confirm_apply set to 'yes'"
          echo ""
          echo "Current confirm_apply value: '${{ github.event.inputs.confirm_apply }}'"
          exit 1
        fi
        echo "âœ… Destroy confirmation validated"
        echo "âš ï¸  Proceeding with cluster destruction..."

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup OpenTofu
      uses: opentofu/setup-opentofu@v1
      with:
        tofu_version: ${{ env.TOFU_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create backend configuration
      run: |
        cat > terraform/backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${{ needs.setup-state-backend.outputs.bucket-name }}"
            key            = "eks-cluster/terraform.tfstate"
            region         = "${{ env.AWS_REGION }}"
            dynamodb_table = "${{ needs.setup-state-backend.outputs.dynamodb-table }}"
            encrypt        = true
          }
        }
        EOF

    - name: Terraform Init
      working-directory: ./terraform
      run: tofu init

    - name: Terraform Destroy
      working-directory: ./terraform
      run: |
        tofu destroy \
          -var="cluster_name=${{ env.CLUSTER_NAME }}" \
          -var="region=${{ env.AWS_REGION }}" \
          -auto-approve

    - name: Cleanup state backend (optional)
      if: github.event.inputs.cleanup_backend == 'true'
      run: |
        echo "âš ï¸ Cleaning up state backend resources..."
        
        # Empty and delete S3 bucket
        aws s3 rm s3://${{ needs.setup-state-backend.outputs.bucket-name }} --recursive
        aws s3api delete-bucket --bucket ${{ needs.setup-state-backend.outputs.bucket-name }}
        
        # Delete DynamoDB table
        aws dynamodb delete-table --table-name ${{ needs.setup-state-backend.outputs.dynamodb-table }}
        
        echo "ğŸ—‘ï¸ State backend resources cleaned up"