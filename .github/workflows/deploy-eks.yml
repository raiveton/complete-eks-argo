# .github/workflows/deploy-eks.yml
name: Deploy EKS Cluster with OpenTofu

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'terraform/**'
      - '.github/workflows/deploy-eks.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - '.github/workflows/deploy-eks.yml'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'plan'
        type: choice
        options:
        - plan
        - apply
        - destroy
        - force-destroy
      confirm_apply:
        description: 'Type "yes" to confirm apply/destroy action'
        required: false
        default: 'no'
        type: string
      cluster_name:
        description: 'Override cluster name (optional)'
        required: false
        type: string
      cleanup_backend:
        description: 'Cleanup state backend after destroy'
        required: false
        default: 'false'
        type: choice
        options:
        - 'true'
        - 'false'

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: raiveton-test
  TF_VERSION: 1.6.6
  TOFU_VERSION: 1.6.0
  # Optional: override bucket name (leave empty for auto-generation)
  STATE_BUCKET_NAME: ""

jobs:
  workflow-info:
    name: Workflow Info
    runs-on: ubuntu-latest
    outputs:
      event-type: ${{ github.event_name }}
      is-manual: ${{ github.event_name == 'workflow_dispatch' }}
      is-push: ${{ github.event_name == 'push' }}
      is-pr: ${{ github.event_name == 'pull_request' }}
    
    steps:
    - name: Display Workflow Info
      run: |
        echo "## üîç Workflow Information" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Event Type | ${{ github.event_name }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Repository | ${{ github.repository }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Branch | ${{ github.ref }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Actor | ${{ github.actor }} |" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "| Manual Action | ${{ github.event.inputs.action }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Confirm Apply | ${{ github.event.inputs.confirm_apply }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Custom Cluster | ${{ github.event.inputs.cluster_name }} |" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Expected Flow:" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event_name }}" = "push" ]; then
          echo "1. Setup State Backend" >> $GITHUB_STEP_SUMMARY
          echo "2. Plan (show changes)" >> $GITHUB_STEP_SUMMARY
          echo "3. Apply (if changes detected + manual approval)" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "1. Setup State Backend" >> $GITHUB_STEP_SUMMARY
          echo "2. Plan only (no apply)" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "1. Setup State Backend" >> $GITHUB_STEP_SUMMARY
          echo "2. ${{ github.event.inputs.action }} (with confirmations if needed)" >> $GITHUB_STEP_SUMMARY
        fi

  setup-state-backend:
    name: Setup State Backend
    runs-on: ubuntu-latest
    needs: workflow-info
    outputs:
      bucket-name: ${{ steps.create-bucket.outputs.bucket-name || steps.use-existing-bucket.outputs.bucket-name }}
      dynamodb-table: ${{ steps.create-dynamodb.outputs.table-name || steps.use-existing-dynamodb.outputs.table-name }}
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Generate bucket name
      id: generate-name
      run: |
        # Use custom bucket name if provided, otherwise generate from repo info
        if [ -n "${{ env.STATE_BUCKET_NAME }}" ]; then
          BUCKET_NAME="${{ env.STATE_BUCKET_NAME }}"
          echo "Using custom bucket name: ${BUCKET_NAME}"
        else
          # Create predictable bucket name based on repo and owner
          REPO_NAME=$(echo "${GITHUB_REPOSITORY}" | cut -d'/' -f2 | tr '[:upper:]' '[:lower:]')
          OWNER_NAME=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          
          # Create stable bucket name
          BUCKET_NAME="tofu-state2-${OWNER_NAME}-${REPO_NAME}"
          echo "Generated bucket name: ${BUCKET_NAME}"
        fi
        
        # Ensure it's valid (lowercase, no underscores, etc.)
        BUCKET_NAME=$(echo $BUCKET_NAME | sed 's/[^a-z0-9-]/-/g' | sed 's/--*/-/g')
        
        echo "bucket-name=${BUCKET_NAME}" >> $GITHUB_OUTPUT
        echo "dynamodb-table=${BUCKET_NAME}-lock" >> $GITHUB_OUTPUT
        echo "Final bucket name: ${BUCKET_NAME}"

    - name: Check if S3 bucket exists
      id: check-bucket
      run: |
        BUCKET_NAME="${{ steps.generate-name.outputs.bucket-name }}"
        
        # Check if bucket exists and is accessible
        if aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null; then
          echo "exists=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Bucket $BUCKET_NAME already exists and is accessible"
        else
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "üì¶ Bucket $BUCKET_NAME does not exist, will create"
        fi

    - name: Create S3 bucket for Terraform state
      id: create-bucket
      if: steps.check-bucket.outputs.exists == 'false'
      run: |
        BUCKET_NAME="${{ steps.generate-name.outputs.bucket-name }}"
        
        # Create bucket with region-specific logic
        if [ "${{ env.AWS_REGION }}" = "us-east-1" ]; then
          # us-east-1 doesn't require LocationConstraint
          aws s3api create-bucket \
            --bucket $BUCKET_NAME \
            --region ${{ env.AWS_REGION }}
        else
          # All other regions require LocationConstraint
          aws s3api create-bucket \
            --bucket $BUCKET_NAME \
            --region ${{ env.AWS_REGION }} \
            --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}
        fi
        
        # Wait a moment for bucket to be fully created
        sleep 5
        
        # Verify bucket was created
        if aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null; then
          echo "‚úÖ Successfully created S3 bucket: $BUCKET_NAME"
        else
          echo "‚ùå Failed to verify bucket creation"
          exit 1
        fi
        
        # Enable versioning
        aws s3api put-bucket-versioning \
          --bucket $BUCKET_NAME \
          --versioning-configuration Status=Enabled
        
        # Block public access
        aws s3api put-public-access-block \
          --bucket $BUCKET_NAME \
          --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
        
        # Enable encryption
        aws s3api put-bucket-encryption \
          --bucket $BUCKET_NAME \
          --server-side-encryption-configuration '{
            "Rules": [
              {
                "ApplyServerSideEncryptionByDefault": {
                  "SSEAlgorithm": "AES256"
                }
              }
            ]
          }'
        
        echo "bucket-name=${BUCKET_NAME}" >> $GITHUB_OUTPUT

    - name: Use existing bucket
      id: use-existing-bucket
      if: steps.check-bucket.outputs.exists == 'true'
      run: |
        BUCKET_NAME="${{ steps.generate-name.outputs.bucket-name }}"
        echo "bucket-name=${BUCKET_NAME}" >> $GITHUB_OUTPUT
        echo "‚ôªÔ∏è Reusing existing S3 bucket: $BUCKET_NAME"
        
        # Verify bucket configuration
        echo "Checking bucket configuration..."
        aws s3api get-bucket-versioning --bucket $BUCKET_NAME || echo "Versioning status unknown"
        aws s3api get-bucket-encryption --bucket $BUCKET_NAME || echo "Encryption status unknown"

    - name: Check if DynamoDB table exists
      id: check-dynamodb
      run: |
        TABLE_NAME="${{ steps.generate-name.outputs.dynamodb-table }}"
        if aws dynamodb describe-table --table-name $TABLE_NAME 2>/dev/null; then
          echo "exists=true" >> $GITHUB_OUTPUT
        else
          echo "exists=false" >> $GITHUB_OUTPUT
        fi

    - name: Create DynamoDB table for state locking
      id: create-dynamodb
      if: steps.check-dynamodb.outputs.exists == 'false'
      run: |
        TABLE_NAME="${{ steps.generate-name.outputs.dynamodb-table }}"
        
        aws dynamodb create-table \
          --table-name $TABLE_NAME \
          --attribute-definitions AttributeName=LockID,AttributeType=S \
          --key-schema AttributeName=LockID,KeyType=HASH \
          --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
        
        # Wait for table to be active
        aws dynamodb wait table-exists --table-name $TABLE_NAME
        
        echo "table-name=${TABLE_NAME}" >> $GITHUB_OUTPUT
        echo "üîí Created DynamoDB table: $TABLE_NAME"

    - name: Use existing DynamoDB table
      id: use-existing-dynamodb
      if: steps.check-dynamodb.outputs.exists == 'true'
      run: |
        TABLE_NAME="${{ steps.generate-name.outputs.dynamodb-table }}"
        echo "table-name=${TABLE_NAME}" >> $GITHUB_OUTPUT
        echo "‚ôªÔ∏è Reusing existing DynamoDB table: $TABLE_NAME"

  plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend]
    if: |
      github.event_name == 'pull_request' || 
      (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'plan') ||
      (github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'))
    outputs:
      plan-exitcode: ${{ steps.plan.outputs.exitcode }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup OpenTofu
      uses: opentofu/setup-opentofu@v1
      with:
        tofu_version: ${{ env.TOFU_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create backend configuration
      run: |
        cat > terraform/backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${{ needs.setup-state-backend.outputs.bucket-name }}"
            key            = "eks-cluster/terraform.tfstate"
            region         = "${{ env.AWS_REGION }}"
            dynamodb_table = "${{ needs.setup-state-backend.outputs.dynamodb-table }}"
            encrypt        = true
          }
        }
        EOF

    - name: Terraform Init
      working-directory: ./terraform
      run: tofu init

    - name: Terraform Validate
      working-directory: ./terraform
      run: tofu validate

    - name: Terraform Plan
      id: plan
      working-directory: ./terraform
      run: |
        tofu plan \
          -var="cluster_name=${{ env.CLUSTER_NAME }}" \
          -var="region=${{ env.AWS_REGION }}" \
          -out=tfplan \
          -detailed-exitcode
      continue-on-error: true

    - name: Upload plan artifact
      uses: actions/upload-artifact@v4
      with:
        name: terraform-plan-${{ github.run_number }}
        path: terraform/tfplan
        retention-days: 5
        compression-level: 6

    - name: Create Plan Summary
      if: always()
      run: |
        echo "## üìã Terraform Plan Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.plan.outputs.exitcode }}" = "0" ]; then
          echo "‚úÖ **No changes detected** - Infrastructure is up to date" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üéØ No apply needed." >> $GITHUB_STEP_SUMMARY
        elif [ "${{ steps.plan.outputs.exitcode }}" = "2" ]; then
          echo "üìù **Changes detected** - Infrastructure will be modified" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üöÄ Ready to apply changes!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps:" >> $GITHUB_STEP_SUMMARY
          echo "- Review the plan above carefully" >> $GITHUB_STEP_SUMMARY
          echo "- If this is a push to main/develop, the **Apply** job will wait for your approval" >> $GITHUB_STEP_SUMMARY
          echo "- Go to the **Apply** job and click **Review deployments** ‚Üí **Approve and deploy**" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Plan failed** - Please check the logs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîç Check the plan output above for errors." >> $GITHUB_STEP_SUMMARY
        fi

  apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend, plan]
    if: |
      needs.plan.outputs.plan-exitcode == '2' && (
        (github.ref == 'refs/heads/main' && github.event_name == 'push') ||
        (github.ref == 'refs/heads/develop' && github.event_name == 'push') ||
        (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply')
      )
    environment: 
      name: production
      url: ${{ steps.cluster-info.outputs.cluster-endpoint }}
    
    steps:
    - name: Manual Approval Required
      run: |
        echo "üéØ **Manual Approval Required**"
        echo ""
        echo "üìã **Plan Summary:**"
        echo "- Plan detected changes (exit code: ${{ needs.plan.outputs.plan-exitcode }})"
        echo "- Infrastructure will be modified"
        echo ""
        echo "‚è≥ **Waiting for approval...**"
        echo "Someone with approval rights needs to:"
        echo "1. Review the plan output above"
        echo "2. Click 'Review deployments' button"
        echo "3. Select 'production' environment" 
        echo "4. Click 'Approve and deploy'"
        echo ""
        echo "üõ°Ô∏è This ensures no infrastructure changes happen without human review."

    - name: Validate manual confirmation
      if: github.event_name == 'workflow_dispatch'
      run: |
        echo "Debug info:"
        echo "Action: '${{ github.event.inputs.action }}'"
        echo "Confirm apply: '${{ github.event.inputs.confirm_apply }}'"
        echo "Event name: '${{ github.event_name }}'"
        echo ""
        
        if [ "${{ github.event.inputs.action }}" = "apply" ] && [ "${{ github.event.inputs.confirm_apply }}" != "yes" ]; then
          echo "‚ùå Apply action requires confirmation. Please set 'confirm_apply' to 'yes'"
          echo "This safety check prevents accidental infrastructure changes."
          exit 1
        fi
        echo "‚úÖ Manual confirmation validated"

    - name: Push Event Info
      if: github.event_name == 'push'
      run: |
        echo "üîÑ **Push Event Detected**"
        echo "Branch: ${{ github.ref }}"
        echo "Event: ${{ github.event_name }}"
        echo "No manual confirmation needed for push events - using Environment protection instead"

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup OpenTofu
      uses: opentofu/setup-opentofu@v1
      with:
        tofu_version: ${{ env.TOFU_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Download plan artifact
      uses: actions/download-artifact@v4
      with:
        name: terraform-plan-${{ github.run_number }}
        path: terraform/
      continue-on-error: true

    - name: Set cluster name
      id: set-cluster-name
      run: |
        if [ -n "${{ github.event.inputs.cluster_name }}" ]; then
          CLUSTER_NAME="${{ github.event.inputs.cluster_name }}"
          echo "Using custom cluster name: $CLUSTER_NAME"
        else
          CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
          echo "Using default cluster name: $CLUSTER_NAME"
        fi
        echo "cluster-name=${CLUSTER_NAME}" >> $GITHUB_OUTPUT

    - name: Create backend configuration
      run: |
        cat > terraform/backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${{ needs.setup-state-backend.outputs.bucket-name }}"
            key            = "eks-cluster/terraform.tfstate"
            region         = "${{ env.AWS_REGION }}"
            dynamodb_table = "${{ needs.setup-state-backend.outputs.dynamodb-table }}"
            encrypt        = true
          }
        }
        EOF

    - name: Terraform Init
      working-directory: ./terraform
      run: tofu init

    - name: Show Terraform State Before Apply
      working-directory: ./terraform
      run: |
        echo "üìä Current Terraform State:"
        tofu state list || echo "No state found (fresh deployment)"
        echo ""

    - name: Terraform Apply
      working-directory: ./terraform
      run: |
        # Set environment variables for better output
        export TF_IN_AUTOMATION=1
        export TF_CLI_ARGS_apply="-parallelism=10"
        
        # Try to use saved plan first, fallback to new plan if needed
        if [ -f "tfplan" ]; then
          echo "üì¶ Using saved plan from previous step"
          echo "üöÄ Starting apply process..."
          echo "‚è±Ô∏è  This may take 15-20 minutes for EKS cluster creation..."
          echo ""
          
          # Apply with progress indicators
          tofu apply -no-color tfplan | while IFS= read -r line; do
            echo "$line"
            # Add timestamps to resource creation
            if [[ $line == *"Creating..."* ]] || [[ $line == *"Creation complete"* ]] || [[ $line == *"Still creating"* ]]; then
              echo "  ‚îî‚îÄ [$(date '+%H:%M:%S')] $line" >&2
            fi
          done
          
          echo ""
          echo "‚úÖ Apply completed successfully!"
        else
          echo "üìù Creating fresh plan (saved plan not found)"
          echo "üöÄ Starting fresh apply process..."
          echo "‚è±Ô∏è  This may take 15-20 minutes for EKS cluster creation..."
          echo ""
          
          # Apply with progress indicators
          tofu apply \
            -var="cluster_name=${{ steps.set-cluster-name.outputs.cluster-name }}" \
            -var="region=${{ env.AWS_REGION }}" \
            -auto-approve \
            -no-color | while IFS= read -r line; do
            echo "$line"
            # Add timestamps to resource creation
            if [[ $line == *"Creating..."* ]] || [[ $line == *"Creation complete"* ]] || [[ $line == *"Still creating"* ]]; then
              echo "  ‚îî‚îÄ [$(date '+%H:%M:%S')] $line" >&2
            fi
          done
          
          echo ""
          echo "‚úÖ Fresh apply completed successfully!"
        fi

    - name: Show Created Resources
      working-directory: ./terraform
      if: always()
      run: |
        echo "üìã Resources in Terraform State:"
        tofu state list | sort
        echo ""
        echo "üîç Key Resource Details:"
        tofu state show aws_eks_cluster.eks_cluster | grep -E "(id|arn|endpoint|status)" || echo "EKS cluster not found in state"

    - name: Get cluster info
      id: cluster-info
      working-directory: ./terraform
      run: |
        CLUSTER_ENDPOINT=$(tofu output -raw cluster_endpoint)
        CLUSTER_NAME=$(tofu output -raw cluster_id)
        
        echo "cluster-endpoint=${CLUSTER_ENDPOINT}" >> $GITHUB_OUTPUT
        echo "cluster-name=${CLUSTER_NAME}" >> $GITHUB_OUTPUT
        
        # Wait a bit for ArgoCD to be ready
        echo "‚è≥ Waiting for ArgoCD LoadBalancer to be ready..."
        sleep 60
        
        # Get ArgoCD info
        ARGOCD_URL=$(tofu output -raw argocd_server_url 2>/dev/null || echo "pending")        
        echo "argocd-url=${ARGOCD_URL}" >> $GITHUB_OUTPUT
        echo "argocd-password=${ARGOCD_PASSWORD}" >> $GITHUB_OUTPUT

    - name: Configure kubectl
      run: |
        aws eks --region ${{ env.AWS_REGION }} update-kubeconfig --name ${{ steps.cluster-info.outputs.cluster-name }}

    - name: Test cluster connectivity
      run: |
        kubectl cluster-info
        kubectl get nodes

    - name: Create deployment summary
      run: |
        echo "## üöÄ EKS Cluster Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "‚úÖ **Successfully deployed!**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Resource | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Cluster Name | ${{ steps.cluster-info.outputs.cluster-name }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Cluster Endpoint | ${{ steps.cluster-info.outputs.cluster-endpoint }} |" >> $GITHUB_STEP_SUMMARY
        echo "| AWS Region | ${{ env.AWS_REGION }} |" >> $GITHUB_STEP_SUMMARY
        echo "| State Bucket | ${{ needs.setup-state-backend.outputs.bucket-name }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Configure kubectl locally:" >> $GITHUB_STEP_SUMMARY
        echo '```bash' >> $GITHUB_STEP_SUMMARY
        echo "aws eks --region ${{ env.AWS_REGION }} update-kubeconfig --name ${{ steps.cluster-info.outputs.cluster-name }}" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY

  force-destroy:
    name: Force Destroy (AWS Direct)
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'force-destroy'
    environment: 
      name: production-destroy
      url: "Force destroying EKS cluster via AWS CLI"
    
    steps:
    - name: Validate destroy confirmation
      run: |
        if [ "${{ github.event.inputs.confirm_apply }}" != "yes" ]; then
          echo "‚ùå Force destroy requires confirmation. Please set 'confirm_apply' to 'yes'"
          echo "‚ö†Ô∏è  This will force delete EKS cluster and resources via AWS CLI!"
          exit 1
        fi
        echo "‚úÖ Force destroy confirmation validated"

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Force delete EKS resources
      run: |
        CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
        
        echo "üîç Checking for existing EKS cluster..."
        if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} 2>/dev/null; then
          echo "üìã Found cluster $CLUSTER_NAME, proceeding with force deletion..."
          
          # Delete node groups first
          echo "üóëÔ∏è Deleting node groups..."
          NODE_GROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query 'nodegroups[]' --output text)
          for ng in $NODE_GROUPS; do
            echo "Deleting node group: $ng"
            aws eks delete-nodegroup --cluster-name $CLUSTER_NAME --nodegroup-name $ng --region ${{ env.AWS_REGION }}
          done
          
          # Wait for node groups to delete
          echo "‚è≥ Waiting for node groups to delete..."
          for ng in $NODE_GROUPS; do
            aws eks wait nodegroup-deleted --cluster-name $CLUSTER_NAME --nodegroup-name $ng --region ${{ env.AWS_REGION }}
          done
          
          # Delete cluster
          echo "üóëÔ∏è Deleting EKS cluster..."
          aws eks delete-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          aws eks wait cluster-deleted --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          
          echo "‚úÖ EKS cluster deleted successfully"
        else
          echo "‚ÑπÔ∏è No EKS cluster found with name $CLUSTER_NAME"
        fi

    - name: Clean state backend
      if: github.event.inputs.cleanup_backend == 'true'
      run: |
        echo "üóëÔ∏è Cleaning state backend..."
        aws s3 rm s3://${{ needs.setup-state-backend.outputs.bucket-name }}/eks-cluster/ --recursive
        echo "‚úÖ State cleaned"

  destroy:
    name: Terraform Destroy
    runs-on: ubuntu-latest
    needs: [workflow-info, setup-state-backend]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    environment: 
      name: production-destroy
      url: "Destroying EKS cluster and all resources"
    
    steps:
    - name: Validate destroy confirmation
      run: |
        echo "Debug info:"
        echo "Action: '${{ github.event.inputs.action }}'"
        echo "Confirm apply: '${{ github.event.inputs.confirm_apply }}'"
        echo "Event name: '${{ github.event_name }}'"
        echo ""
        
        if [ "${{ github.event.inputs.confirm_apply }}" != "yes" ]; then
          echo "‚ùå Destroy action requires confirmation. Please set 'confirm_apply' to 'yes'"
          echo "‚ö†Ô∏è  This will permanently delete your EKS cluster and all associated resources!"
          echo "üí° To confirm, run the workflow again with confirm_apply set to 'yes'"
          echo ""
          echo "Current confirm_apply value: '${{ github.event.inputs.confirm_apply }}'"
          exit 1
        fi
        echo "‚úÖ Destroy confirmation validated"
        echo "‚ö†Ô∏è  Proceeding with cluster destruction..."

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup OpenTofu
      uses: opentofu/setup-opentofu@v1
      with:
        tofu_version: ${{ env.TOFU_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create backend configuration
      run: |
        cat > terraform/backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${{ needs.setup-state-backend.outputs.bucket-name }}"
            key            = "eks-cluster/terraform.tfstate"
            region         = "${{ env.AWS_REGION }}"
            dynamodb_table = "${{ needs.setup-state-backend.outputs.dynamodb-table }}"
            encrypt        = true
          }
        }
        EOF

    - name: Terraform Init
      working-directory: ./terraform
      run: tofu init

    - name: Terraform Destroy
      working-directory: ./terraform
      run: |
        tofu destroy \
          -var="cluster_name=${{ env.CLUSTER_NAME }}" \
          -var="region=${{ env.AWS_REGION }}" \
          -auto-approve

    - name: Cleanup state backend (optional)
      if: github.event.inputs.cleanup_backend == 'true'
      run: |
        echo "‚ö†Ô∏è Cleaning up state backend resources..."
        
        # Empty and delete S3 bucket
        aws s3 rm s3://${{ needs.setup-state-backend.outputs.bucket-name }} --recursive
        aws s3api delete-bucket --bucket ${{ needs.setup-state-backend.outputs.bucket-name }}
        
        # Delete DynamoDB table
        aws dynamodb delete-table --table-name ${{ needs.setup-state-backend.outputs.dynamodb-table }}
        
        echo "üóëÔ∏è State backend resources cleaned up"